{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","### In this notebook we use [PointNet](https://arxiv.org/abs/1612.00593) to perform 3D Object Classification on [ModelNet10 Dataset](http://modelnet.cs.princeton.edu/#)."]},{"cell_type":"markdown","metadata":{},"source":["<h3><center>Applications of PointNet</center></h3>\n","<img src=\"http://stanford.edu/~rqi/pointnet/images/teaser.jpg\" width=\"600\" height=\"500\"/>\n","<h4></h4>\n","<h4><center><a href=\"https://arxiv.org/pdf/1612.00593.pdf\">Source: PointNet [Charles R. Qi et. al.]</a></center></h4>"]},{"cell_type":"markdown","metadata":{},"source":["## Acknowledgements\n","\n","### This work was inspired by and borrows code from [Nikita Karaev's](https://github.com/nikitakaraevv) [PointNet implementation](https://github.com/nikitakaraevv/pointnet)."]},{"cell_type":"markdown","metadata":{},"source":["### Libraries 📚⬇"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import itertools\n","import math, random\n","random.seed = 42\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","from pathlib import Path\n","import scipy.spatial.distance\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!kaggle datasets download -d balraj98/modelnet10-princeton-3d-object-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile(\"modelnet10-princeton-3d-object-dataset.zip\", mode=\"r\") as data:\n","     data.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.remove(\"modelnet10-princeton-3d-object-dataset.zip\")"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["{'bathtub': 0,\n"," 'bed': 1,\n"," 'chair': 2,\n"," 'desk': 3,\n"," 'dresser': 4,\n"," 'monitor': 5,\n"," 'night_stand': 6,\n"," 'sofa': 7,\n"," 'table': 8,\n"," 'toilet': 9}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["path = Path(\"ModelNet10\")\n","\n","folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n","classes = {folder: i for i, folder in enumerate(folders)};\n","classes"]},{"cell_type":"markdown","metadata":{},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def read_off(file):\n","    if 'OFF' != file.readline().strip():\n","        raise('Not a valid OFF header')\n","    n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n","    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n","    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n","    return verts, faces\n","\n","\n","def visualize_rotate(data):\n","    x_eye, y_eye, z_eye = 1.25, 1.25, 0.8\n","    frames=[]\n","\n","    def rotate_z(x, y, z, theta):\n","        w = x+1j*y\n","        return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n","\n","    for t in np.arange(0, 10.26, 0.1):\n","        xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n","        frames.append(dict(layout=dict(scene=dict(camera=dict(eye=dict(x=xe, y=ye, z=ze))))))\n","    fig = go.Figure(data=data,\n","        layout=go.Layout(\n","            updatemenus=[dict(type='buttons',\n","                showactive=False,\n","                y=1,\n","                x=0.8,\n","                xanchor='left',\n","                yanchor='bottom',\n","                pad=dict(t=45, r=10),\n","                buttons=[dict(label='Play',\n","                    method='animate',\n","                    args=[None, dict(frame=dict(duration=50, redraw=True),\n","                        transition=dict(duration=0),\n","                        fromcurrent=True,\n","                        mode='immediate'\n","                        )]\n","                    )\n","                ])]\n","        ),\n","        frames=frames\n","    )\n","\n","    return fig\n","\n","\n","def pcshow(xs,ys,zs):\n","    data=[go.Scatter3d(x=xs, y=ys, z=zs,\n","                                   mode='markers')]\n","    fig = visualize_rotate(data)\n","    fig.update_traces(marker=dict(size=2,\n","                      line=dict(width=2,\n","                      color='DarkSlateGrey')),\n","                      selector=dict(mode='markers'))\n","    fig.show()"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["6433"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["with open(path/\"sofa/train/sofa_0002.off\", 'r') as f:\n","    verts, faces = read_off(f)\n","    \n","i,j,k = np.array(faces).T\n","x,y,z = np.array(verts).T\n","len(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize_rotate([go.Mesh3d(x=x, y=y, z=z, color='yellowgreen', opacity=0.50, i=i,j=j,k=k)]).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize_rotate([go.Scatter3d(x=x, y=y, z=z, mode='markers')]).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pcshow(x,y,z)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["class PointSampler(object):\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, int)\n","        self.output_size = output_size\n","    \n","    def triangle_area(self, pt1, pt2, pt3):\n","        side_a = np.linalg.norm(pt1 - pt2)\n","        side_b = np.linalg.norm(pt2 - pt3)\n","        side_c = np.linalg.norm(pt3 - pt1)\n","        s = 0.5 * ( side_a + side_b + side_c)\n","        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n","\n","    def sample_point(self, pt1, pt2, pt3):\n","        # barycentric coordinates on a triangle\n","        # https://mathworld.wolfram.com/BarycentricCoordinates.html\n","        s, t = sorted([random.random(), random.random()])\n","        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n","        return (f(0), f(1), f(2))\n","        \n","    \n","    def __call__(self, mesh):\n","        verts, faces = mesh\n","        verts = np.array(verts)\n","        areas = np.zeros((len(faces)))\n","\n","        for i in range(len(areas)):\n","            areas[i] = (self.triangle_area(verts[faces[i][0]],\n","                                           verts[faces[i][1]],\n","                                           verts[faces[i][2]]))\n","            \n","        sampled_faces = (random.choices(faces, \n","                                      weights=areas,\n","                                      cum_weights=None,\n","                                      k=self.output_size))\n","        \n","        sampled_points = np.zeros((self.output_size, 3))\n","\n","        for i in range(len(sampled_faces)):\n","            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],\n","                                                   verts[sampled_faces[i][1]],\n","                                                   verts[sampled_faces[i][2]]))\n","        \n","        return sampled_points"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pointcloud = PointSampler(3000)((verts, faces))\n","pcshow(*pointcloud.T)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["class Normalize(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","        \n","        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n","        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n","\n","        return  norm_pointcloud"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pointcloud' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m norm_pointcloud \u001b[39m=\u001b[39m Normalize()(pointcloud)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pcshow(\u001b[39m*\u001b[39mnorm_pointcloud\u001b[39m.\u001b[39mT)\n","\u001b[0;31mNameError\u001b[0m: name 'pointcloud' is not defined"]}],"source":["norm_pointcloud = Normalize()(pointcloud)\n","pcshow(*norm_pointcloud.T)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["class RandRotation_z(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","\n","        theta = random.random() * 2. * math.pi\n","        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n","                               [ math.sin(theta),  math.cos(theta),    0],\n","                               [0,                             0,      1]])\n","        \n","        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n","        return  rot_pointcloud\n","    \n","class RandomNoise(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","\n","        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n","    \n","        noisy_pointcloud = pointcloud + noise\n","        return  noisy_pointcloud"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'norm_pointcloud' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rot_pointcloud \u001b[39m=\u001b[39m RandRotation_z()(norm_pointcloud)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m noisy_rot_pointcloud \u001b[39m=\u001b[39m RandomNoise()(rot_pointcloud)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pcshow(\u001b[39m*\u001b[39mnoisy_rot_pointcloud\u001b[39m.\u001b[39mT)\n","\u001b[0;31mNameError\u001b[0m: name 'norm_pointcloud' is not defined"]}],"source":["rot_pointcloud = RandRotation_z()(norm_pointcloud)\n","noisy_rot_pointcloud = RandomNoise()(rot_pointcloud)\n","pcshow(*noisy_rot_pointcloud.T)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["class ToTensor(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","\n","        return torch.from_numpy(pointcloud)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def default_transforms():\n","    return transforms.Compose([\n","                                PointSampler(1024),\n","                                Normalize(),\n","                                ToTensor()\n","                              ])"]},{"cell_type":"markdown","metadata":{},"source":["### Define Datasets / Dataloaders"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["class PointCloudData(Dataset):\n","    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n","        self.root_dir = root_dir\n","        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n","        self.classes = {folder: i for i, folder in enumerate(folders)}\n","        self.transforms = transform if not valid else default_transforms()\n","        self.valid = valid\n","        self.files = []\n","        for category in self.classes.keys():\n","            new_dir = root_dir/Path(category)/folder\n","            for file in os.listdir(new_dir):\n","                if file.endswith('.off'):\n","                    sample = {}\n","                    sample['pcd_path'] = new_dir/file\n","                    sample['category'] = category\n","                    self.files.append(sample)\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __preproc__(self, file):\n","        verts, faces = read_off(file)\n","        if self.transforms:\n","            pointcloud = self.transforms((verts, faces))\n","        return pointcloud\n","\n","    def __getitem__(self, idx):\n","        pcd_path = self.files[idx]['pcd_path']\n","        category = self.files[idx]['category']\n","        with open(pcd_path, 'r') as f:\n","            pointcloud = self.__preproc__(f)\n","        return {'pointcloud': pointcloud, \n","                'category': self.classes[category]}"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["train_transforms = transforms.Compose([\n","                    PointSampler(1024),\n","                    Normalize(),\n","                    RandRotation_z(),\n","                    RandomNoise(),\n","                    ToTensor()\n","                    ])"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["train_ds = PointCloudData(path, transform=train_transforms)\n","valid_ds = PointCloudData(path, valid=True, folder='test', transform=train_transforms)"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'bathtub',\n"," 1: 'bed',\n"," 2: 'chair',\n"," 3: 'desk',\n"," 4: 'dresser',\n"," 5: 'monitor',\n"," 6: 'night_stand',\n"," 7: 'sofa',\n"," 8: 'table',\n"," 9: 'toilet'}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["inv_classes = {i: cat for cat, i in train_ds.classes.items()};\n","inv_classes"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size:  3991\n","Valid dataset size:  908\n","Number of classes:  10\n","Sample pointcloud shape:  torch.Size([1024, 3])\n","Class:  bathtub\n"]}],"source":["print('Train dataset size: ', len(train_ds))\n","print('Valid dataset size: ', len(valid_ds))\n","print('Number of classes: ', len(train_ds.classes))\n","print('Sample pointcloud shape: ', train_ds[0]['pointcloud'].size())\n","print('Class: ', inv_classes[train_ds[0]['category']])"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["train_loader = DataLoader(dataset=train_ds, batch_size=8, shuffle=True)\n","valid_loader = DataLoader(dataset=valid_ds, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["<h3><center>PointNet Model Architecture</center></h3>\n","<img src=\"http://stanford.edu/~rqi/pointnet/images/pointnet.jpg\" width=\"750\" height=\"750\"/>\n","<h4></h4>\n","<h4><center><a href=\"https://arxiv.org/pdf/1612.00593.pdf\">Source: PointNet [Charles R. Qi et. al.]</a></center></h4>"]},{"cell_type":"markdown","metadata":{},"source":["### Model Definition"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","\n","class Tnet(nn.Module):\n","    def __init__(self, k=3):\n","        super().__init__()\n","        self.k=k\n","        self.conv1 = nn.Conv1d(k,64,1)\n","        self.conv2 = nn.Conv1d(64,128,1)\n","        self.conv3 = nn.Conv1d(128,1024,1)\n","        self.fc1 = nn.Linear(1024,512)\n","        self.fc2 = nn.Linear(512,256)\n","        self.fc3 = nn.Linear(256,k*k)\n","\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(1024)\n","        self.bn4 = nn.BatchNorm1d(512)\n","        self.bn5 = nn.BatchNorm1d(256)\n","\n","\n","    def forward(self, input):\n","        # input.shape == (bs,n,3)\n","        bs = input.size(0)\n","        xb = F.relu(self.bn1(self.conv1(input)))\n","        xb = F.relu(self.bn2(self.conv2(xb)))\n","        xb = F.relu(self.bn3(self.conv3(xb)))\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        flat = nn.Flatten(1)(pool)\n","        xb = F.relu(self.bn4(self.fc1(flat)))\n","        xb = F.relu(self.bn5(self.fc2(xb)))\n","\n","        #initialize as identity\n","        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n","        if xb.is_cuda:\n","            init=init.cuda()\n","        matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n","        return matrix\n","\n","\n","class Transform(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_transform = Tnet(k=3)\n","        self.feature_transform = Tnet(k=64)\n","        self.conv1 = nn.Conv1d(3,64,1)\n","\n","        self.conv2 = nn.Conv1d(64,128,1)\n","        self.conv3 = nn.Conv1d(128,1024,1)\n","\n","\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.bn3 = nn.BatchNorm1d(1024)\n","\n","    def forward(self, input):\n","        matrix3x3 = self.input_transform(input)\n","        # batch matrix multiplication\n","        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","\n","        xb = F.relu(self.bn1(self.conv1(xb)))\n","\n","        matrix64x64 = self.feature_transform(xb)\n","        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n","\n","        xb = F.relu(self.bn2(self.conv2(xb)))\n","        xb = self.bn3(self.conv3(xb))\n","        xb = nn.MaxPool1d(xb.size(-1))(xb)\n","        output = nn.Flatten(1)(xb)\n","        return output, matrix3x3, matrix64x64\n","\n","class PointNet(nn.Module):\n","    def __init__(self, classes = 10):\n","        super().__init__()\n","        self.transform = Transform()\n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, classes)\n","        \n","\n","        self.bn1 = nn.BatchNorm1d(512)\n","        self.bn2 = nn.BatchNorm1d(256)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input):\n","        xb, matrix3x3, matrix64x64 = self.transform(input)\n","        xb = F.relu(self.bn1(self.fc1(xb)))\n","        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n","        output = self.fc3(xb)\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n","    criterion = torch.nn.NLLLoss()\n","    bs=outputs.size(0)\n","    id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n","    id64x64 = torch.eye(64, requires_grad=True).repeat(bs,1,1)\n","    if outputs.is_cuda:\n","        id3x3=id3x3.cuda()\n","        id64x64=id64x64.cuda()\n","    diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n","    diff64x64 = id64x64-torch.bmm(m64x64,m64x64.transpose(1,2))\n","    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3)+torch.norm(diff64x64)) / float(bs)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["pointnet = PointNet()\n","pointnet.to(device);\n","\n","# Load a pre-trained model if it exists\n","#pointnet.load_state_dict(torch.load('../input/pointnet-for-3d-object-classification-pytorch/save.pth'))"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.00025)"]},{"cell_type":"markdown","metadata":{},"source":["### Train PointNet"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["def train(model, train_loader, val_loader=None,  epochs=1):\n","    for epoch in range(epochs): \n","        pointnet.train()\n","        running_loss = 0.0\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n","            optimizer.zero_grad()\n","            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n","\n","            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 5 == 4:    # print every 10 mini-batches\n","                print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n","                    (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n","                running_loss = 0.0\n","\n","        pointnet.eval()\n","        correct = total = 0\n","\n","        # validation\n","        if val_loader:\n","            with torch.no_grad():\n","                for data in val_loader:\n","                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n","                    outputs, __, __ = pointnet(inputs.transpose(1,2))\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","            val_acc = 100. * correct / total\n","            print('Valid accuracy: %d %%' % val_acc)\n","\n","        # save the model\n","        torch.save(pointnet.state_dict(), \"save.pth\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Epoch: 1, Batch:    5 /  499], loss: 0.991\n","[Epoch: 1, Batch:   10 /  499], loss: 1.031\n","[Epoch: 1, Batch:   15 /  499], loss: 0.914\n","[Epoch: 1, Batch:   20 /  499], loss: 0.855\n","[Epoch: 1, Batch:   25 /  499], loss: 0.915\n","[Epoch: 1, Batch:   30 /  499], loss: 0.997\n","[Epoch: 1, Batch:   35 /  499], loss: 0.948\n","[Epoch: 1, Batch:   40 /  499], loss: 0.899\n","[Epoch: 1, Batch:   45 /  499], loss: 0.891\n","[Epoch: 1, Batch:   50 /  499], loss: 1.058\n","[Epoch: 1, Batch:   55 /  499], loss: 0.919\n","[Epoch: 1, Batch:   60 /  499], loss: 0.965\n","[Epoch: 1, Batch:   65 /  499], loss: 0.853\n","[Epoch: 1, Batch:   70 /  499], loss: 0.819\n","[Epoch: 1, Batch:   75 /  499], loss: 0.842\n","[Epoch: 1, Batch:   80 /  499], loss: 0.927\n","[Epoch: 1, Batch:   85 /  499], loss: 0.866\n","[Epoch: 1, Batch:   90 /  499], loss: 0.804\n","[Epoch: 1, Batch:   95 /  499], loss: 0.956\n","[Epoch: 1, Batch:  100 /  499], loss: 0.861\n","[Epoch: 1, Batch:  105 /  499], loss: 0.955\n","[Epoch: 1, Batch:  110 /  499], loss: 0.898\n","[Epoch: 1, Batch:  115 /  499], loss: 0.791\n","[Epoch: 1, Batch:  120 /  499], loss: 0.827\n","[Epoch: 1, Batch:  125 /  499], loss: 0.876\n","[Epoch: 1, Batch:  130 /  499], loss: 0.858\n","[Epoch: 1, Batch:  135 /  499], loss: 0.869\n","[Epoch: 1, Batch:  140 /  499], loss: 0.840\n","[Epoch: 1, Batch:  145 /  499], loss: 0.729\n","[Epoch: 1, Batch:  150 /  499], loss: 0.811\n","[Epoch: 1, Batch:  155 /  499], loss: 0.811\n","[Epoch: 1, Batch:  160 /  499], loss: 0.711\n","[Epoch: 1, Batch:  165 /  499], loss: 0.888\n","[Epoch: 1, Batch:  170 /  499], loss: 0.778\n","[Epoch: 1, Batch:  175 /  499], loss: 0.763\n","[Epoch: 1, Batch:  180 /  499], loss: 0.814\n","[Epoch: 1, Batch:  185 /  499], loss: 0.638\n","[Epoch: 1, Batch:  190 /  499], loss: 0.791\n","[Epoch: 1, Batch:  195 /  499], loss: 0.788\n","[Epoch: 1, Batch:  200 /  499], loss: 0.745\n","[Epoch: 1, Batch:  205 /  499], loss: 0.630\n","[Epoch: 1, Batch:  210 /  499], loss: 0.553\n","[Epoch: 1, Batch:  215 /  499], loss: 0.662\n","[Epoch: 1, Batch:  220 /  499], loss: 0.634\n","[Epoch: 1, Batch:  225 /  499], loss: 0.805\n","[Epoch: 1, Batch:  230 /  499], loss: 0.733\n","[Epoch: 1, Batch:  235 /  499], loss: 0.656\n","[Epoch: 1, Batch:  240 /  499], loss: 0.583\n","[Epoch: 1, Batch:  245 /  499], loss: 0.599\n","[Epoch: 1, Batch:  250 /  499], loss: 0.623\n","[Epoch: 1, Batch:  255 /  499], loss: 0.785\n","[Epoch: 1, Batch:  260 /  499], loss: 0.734\n","[Epoch: 1, Batch:  265 /  499], loss: 0.644\n","[Epoch: 1, Batch:  270 /  499], loss: 0.754\n","[Epoch: 1, Batch:  275 /  499], loss: 0.659\n","[Epoch: 1, Batch:  280 /  499], loss: 0.642\n","[Epoch: 1, Batch:  285 /  499], loss: 0.631\n","[Epoch: 1, Batch:  290 /  499], loss: 0.733\n","[Epoch: 1, Batch:  295 /  499], loss: 0.642\n","[Epoch: 1, Batch:  300 /  499], loss: 0.567\n","[Epoch: 1, Batch:  305 /  499], loss: 0.753\n","[Epoch: 1, Batch:  310 /  499], loss: 0.737\n","[Epoch: 1, Batch:  315 /  499], loss: 0.529\n","[Epoch: 1, Batch:  320 /  499], loss: 0.767\n","[Epoch: 1, Batch:  325 /  499], loss: 0.780\n","[Epoch: 1, Batch:  330 /  499], loss: 0.749\n","[Epoch: 1, Batch:  335 /  499], loss: 0.780\n","[Epoch: 1, Batch:  340 /  499], loss: 0.725\n","[Epoch: 1, Batch:  345 /  499], loss: 0.560\n","[Epoch: 1, Batch:  350 /  499], loss: 0.607\n","[Epoch: 1, Batch:  355 /  499], loss: 0.591\n","[Epoch: 1, Batch:  360 /  499], loss: 0.897\n","[Epoch: 1, Batch:  365 /  499], loss: 0.826\n","[Epoch: 1, Batch:  370 /  499], loss: 0.647\n","[Epoch: 1, Batch:  375 /  499], loss: 0.794\n","[Epoch: 1, Batch:  380 /  499], loss: 0.676\n","[Epoch: 1, Batch:  385 /  499], loss: 0.602\n","[Epoch: 1, Batch:  390 /  499], loss: 0.657\n","[Epoch: 1, Batch:  395 /  499], loss: 0.650\n","[Epoch: 1, Batch:  400 /  499], loss: 0.612\n","[Epoch: 1, Batch:  405 /  499], loss: 0.543\n","[Epoch: 1, Batch:  410 /  499], loss: 0.451\n","[Epoch: 1, Batch:  415 /  499], loss: 0.671\n","[Epoch: 1, Batch:  420 /  499], loss: 0.727\n","[Epoch: 1, Batch:  425 /  499], loss: 0.616\n","[Epoch: 1, Batch:  430 /  499], loss: 0.637\n","[Epoch: 1, Batch:  435 /  499], loss: 0.461\n","[Epoch: 1, Batch:  440 /  499], loss: 0.922\n","[Epoch: 1, Batch:  445 /  499], loss: 0.638\n","[Epoch: 1, Batch:  450 /  499], loss: 0.589\n","[Epoch: 1, Batch:  455 /  499], loss: 0.665\n","[Epoch: 1, Batch:  460 /  499], loss: 0.649\n","[Epoch: 1, Batch:  465 /  499], loss: 0.452\n","[Epoch: 1, Batch:  470 /  499], loss: 0.557\n","[Epoch: 1, Batch:  475 /  499], loss: 0.603\n","[Epoch: 1, Batch:  480 /  499], loss: 0.812\n","[Epoch: 1, Batch:  485 /  499], loss: 0.681\n","[Epoch: 1, Batch:  490 /  499], loss: 0.670\n","[Epoch: 1, Batch:  495 /  499], loss: 0.715\n","Valid accuracy: 63 %\n"]}],"source":["train(pointnet, train_loader, valid_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Test PointNet"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch [   1 /   15]\n"]},{"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 43\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBatch [\u001b[39m\u001b[39m%4d\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m%4d\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(valid_loader)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mpointcloud\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat(), data[\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs, __, __ \u001b[39m=\u001b[39m pointnet(inputs\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m all_preds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(preds\u001b[39m.\u001b[39mnumpy())\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 43\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     xb, matrix3x3, matrix64x64 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     xb \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(xb)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     xb \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(xb))))\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 43\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     matrix3x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_transform(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# batch matrix multiplication\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     xb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(torch\u001b[39m.\u001b[39mtranspose(\u001b[39minput\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), matrix3x3)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/dmitry/Рабочий стол/ClassificationPointNet3D/notebooka1054845b4.ipynb Ячейка 43\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# input.shape == (bs,n,3)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     bs \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     xb \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39minput\u001b[39;49m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     xb \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(xb)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dmitry/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/ClassificationPointNet3D/notebooka1054845b4.ipynb#X60sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     xb \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(xb)))\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/Рабочий стол/ClassificationPointNet3D/conda_venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"]}],"source":["pointnet.eval()\n","all_preds = []\n","all_labels = []\n","with torch.no_grad():\n","    for i, data in enumerate(valid_loader):\n","        print('Batch [%4d / %4d]' % (i+1, len(valid_loader)))\n","        \n","        inputs, labels = data['pointcloud'].float(), data['category']\n","        outputs, __, __ = pointnet(inputs.transpose(1,2))\n","        _, preds = torch.max(outputs.data, 1)\n","        all_preds += list(preds.numpy())\n","        all_labels += list(labels.numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(all_labels, all_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# function from https://deeplizard.com/learn/video/0LhiS6yu2qQ\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(8,8))\n","plot_confusion_matrix(cm, list(classes.keys()), normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(8,8))\n","plot_confusion_matrix(cm, list(classes.keys()), normalize=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":943153,"sourceId":1598438,"sourceType":"datasetVersion"},{"sourceId":52279457,"sourceType":"kernelVersion"}],"dockerImageVersionId":30019,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
